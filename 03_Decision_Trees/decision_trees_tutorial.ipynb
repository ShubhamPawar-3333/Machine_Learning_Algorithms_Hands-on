{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Trees - Hands-On Tutorial\n",
                "\n",
                "This notebook provides a comprehensive guide to Decision Trees with practical examples.\n",
                "\n",
                "## Contents\n",
                "1. Import Libraries\n",
                "2. Load and Explore Dataset\n",
                "3. Data Preprocessing\n",
                "4. Decision Tree Classifier\n",
                "5. Visualizing the Tree\n",
                "6. Feature Importance\n",
                "7. Hyperparameter Tuning\n",
                "8. Preventing Overfitting\n",
                "9. Decision Tree Regressor\n",
                "10. Implementation from Scratch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data manipulation\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Machine Learning\n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "\n",
                "# Settings\n",
                "plt.style.use('seaborn-v0_8')\n",
                "sns.set_palette('husl')\n",
                "%matplotlib inline\n",
                "\n",
                "# Ignore warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load and Explore Dataset\n",
                "\n",
                "We'll use the Iris dataset for classification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import load_iris\n",
                "\n",
                "# Load dataset\n",
                "iris = load_iris()\n",
                "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
                "df['species'] = iris.target\n",
                "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
                "\n",
                "print(\"Dataset Shape:\", df.shape)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset information\n",
                "print(\"Dataset Info:\")\n",
                "print(df.info())\n",
                "print(\"\\nClass Distribution:\")\n",
                "print(df['species_name'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature distributions\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.ravel()\n",
                "\n",
                "for idx, col in enumerate(iris.feature_names):\n",
                "    for species in df['species_name'].unique():\n",
                "        subset = df[df['species_name'] == species]\n",
                "        axes[idx].hist(subset[col], alpha=0.6, label=species, bins=15)\n",
                "    axes[idx].set_xlabel(col)\n",
                "    axes[idx].set_ylabel('Frequency')\n",
                "    axes[idx].legend()\n",
                "    axes[idx].set_title(f'Distribution of {col}')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate features and target\n",
                "X = iris.data\n",
                "y = iris.target\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")\n",
                "print(f\"\\nClass distribution in training set:\")\n",
                "print(pd.Series(y_train).value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Decision Tree Classifier\n",
                "\n",
                "### 4.1 Train a Simple Decision Tree"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create and train decision tree\n",
                "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
                "dt_classifier.fit(X_train, y_train)\n",
                "\n",
                "# Make predictions\n",
                "y_pred = dt_classifier.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Accuracy: {accuracy:.4f}\")\n",
                "print(f\"\\nTree Depth: {dt_classifier.get_depth()}\")\n",
                "print(f\"Number of Leaves: {dt_classifier.get_n_leaves()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed classification report\n",
                "print(\"Classification Report:\")\n",
                "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion Matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title('Confusion Matrix - Decision Tree')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualizing the Tree\n",
                "\n",
                "One of the biggest advantages of Decision Trees is their interpretability!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the decision tree\n",
                "plt.figure(figsize=(20, 10))\n",
                "plot_tree(dt_classifier, \n",
                "          feature_names=iris.feature_names,\n",
                "          class_names=iris.target_names,\n",
                "          filled=True,\n",
                "          rounded=True,\n",
                "          fontsize=10)\n",
                "plt.title('Decision Tree Visualization', fontsize=16, fontweight='bold')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Tree Visualization\n",
                "\n",
                "- **Top node (root)**: First decision based on most informative feature\n",
                "- **Gini**: Measure of impurity (0 = pure, 0.5 = maximum impurity for binary)\n",
                "- **Samples**: Number of samples at this node\n",
                "- **Value**: Number of samples per class\n",
                "- **Class**: Predicted class at this node"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Feature Importance\n",
                "\n",
                "Decision Trees provide feature importance scores."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature importances\n",
                "importances = dt_classifier.feature_importances_\n",
                "feature_importance_df = pd.DataFrame({\n",
                "    'Feature': iris.feature_names,\n",
                "    'Importance': importances\n",
                "}).sort_values('Importance', ascending=False)\n",
                "\n",
                "print(\"Feature Importance:\")\n",
                "print(feature_importance_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
                "plt.xlabel('Importance')\n",
                "plt.title('Feature Importance - Decision Tree')\n",
                "plt.gca().invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Hyperparameter Tuning\n",
                "\n",
                "### 7.1 Important Hyperparameters\n",
                "\n",
                "- **max_depth**: Maximum depth of the tree\n",
                "- **min_samples_split**: Minimum samples required to split a node\n",
                "- **min_samples_leaf**: Minimum samples required in a leaf node\n",
                "- **max_features**: Number of features to consider for best split\n",
                "- **criterion**: Splitting criterion ('gini' or 'entropy')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Grid Search for best hyperparameters\n",
                "param_grid = {\n",
                "    'max_depth': [2, 3, 4, 5, 6, 7, 8, None],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4],\n",
                "    'criterion': ['gini', 'entropy']\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
                "                          param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(\"Best Parameters:\")\n",
                "print(grid_search.best_params_)\n",
                "print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate best model\n",
                "best_dt = grid_search.best_estimator_\n",
                "y_pred_best = best_dt.predict(X_test)\n",
                "\n",
                "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
                "print(f\"Tree Depth: {best_dt.get_depth()}\")\n",
                "print(f\"Number of Leaves: {best_dt.get_n_leaves()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Preventing Overfitting\n",
                "\n",
                "### 8.1 Comparing Different max_depth Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare different max_depth values\n",
                "depths = range(1, 15)\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for depth in depths:\n",
                "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
                "    dt.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores.append(dt.score(X_train, y_train))\n",
                "    test_scores.append(dt.score(X_test, y_test))\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(depths, train_scores, marker='o', label='Training Score', linewidth=2)\n",
                "plt.plot(depths, test_scores, marker='s', label='Test Score', linewidth=2)\n",
                "plt.xlabel('Max Depth')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.title('Training vs Test Accuracy by Tree Depth')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Optimal max_depth: {depths[np.argmax(test_scores)]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2 Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-validation scores\n",
                "dt_cv = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "cv_scores = cross_val_score(dt_cv, X_train, y_train, cv=5, scoring='accuracy')\n",
                "\n",
                "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
                "print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Decision Tree Regressor\n",
                "\n",
                "Decision Trees can also be used for regression tasks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import fetch_california_housing\n",
                "\n",
                "# Load regression dataset\n",
                "housing = fetch_california_housing()\n",
                "X_reg = housing.data[:1000]  # Use subset for faster training\n",
                "y_reg = housing.target[:1000]\n",
                "\n",
                "# Split data\n",
                "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
                "    X_reg, y_reg, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Train decision tree regressor\n",
                "dt_regressor = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
                "dt_regressor.fit(X_train_reg, y_train_reg)\n",
                "\n",
                "# Predictions\n",
                "y_pred_reg = dt_regressor.predict(X_test_reg)\n",
                "\n",
                "# Evaluate\n",
                "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
                "rmse = np.sqrt(mse)\n",
                "r2 = r2_score(y_test_reg, y_pred_reg)\n",
                "\n",
                "print(f\"RÂ² Score: {r2:.4f}\")\n",
                "print(f\"RMSE: {rmse:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(y_test_reg, y_pred_reg, alpha=0.6)\n",
                "plt.plot([y_test_reg.min(), y_test_reg.max()], \n",
                "         [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
                "plt.xlabel('Actual Values')\n",
                "plt.ylabel('Predicted Values')\n",
                "plt.title('Decision Tree Regressor: Actual vs Predicted')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Implementation from Scratch\n",
                "\n",
                "Understanding how Decision Trees work internally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleDecisionTree:\n",
                "    \"\"\"Simple Decision Tree implementation for educational purposes.\"\"\"\n",
                "    \n",
                "    def __init__(self, max_depth=5):\n",
                "        self.max_depth = max_depth\n",
                "        self.tree = None\n",
                "    \n",
                "    def gini_impurity(self, y):\n",
                "        \"\"\"Calculate Gini impurity.\"\"\"\n",
                "        _, counts = np.unique(y, return_counts=True)\n",
                "        probabilities = counts / len(y)\n",
                "        return 1 - np.sum(probabilities ** 2)\n",
                "    \n",
                "    def split_data(self, X, y, feature_idx, threshold):\n",
                "        \"\"\"Split data based on feature and threshold.\"\"\"\n",
                "        left_mask = X[:, feature_idx] < threshold\n",
                "        right_mask = ~left_mask\n",
                "        return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
                "    \n",
                "    def find_best_split(self, X, y):\n",
                "        \"\"\"Find the best feature and threshold to split on.\"\"\"\n",
                "        best_gini = float('inf')\n",
                "        best_feature = None\n",
                "        best_threshold = None\n",
                "        \n",
                "        n_features = X.shape[1]\n",
                "        \n",
                "        for feature_idx in range(n_features):\n",
                "            thresholds = np.unique(X[:, feature_idx])\n",
                "            \n",
                "            for threshold in thresholds:\n",
                "                _, _, y_left, y_right = self.split_data(X, y, feature_idx, threshold)\n",
                "                \n",
                "                if len(y_left) == 0 or len(y_right) == 0:\n",
                "                    continue\n",
                "                \n",
                "                # Weighted Gini impurity\n",
                "                n = len(y)\n",
                "                gini = (len(y_left) / n) * self.gini_impurity(y_left) + \\\n",
                "                       (len(y_right) / n) * self.gini_impurity(y_right)\n",
                "                \n",
                "                if gini < best_gini:\n",
                "                    best_gini = gini\n",
                "                    best_feature = feature_idx\n",
                "                    best_threshold = threshold\n",
                "        \n",
                "        return best_feature, best_threshold\n",
                "    \n",
                "    def build_tree(self, X, y, depth=0):\n",
                "        \"\"\"Recursively build the decision tree.\"\"\"\n",
                "        # Stopping criteria\n",
                "        if depth >= self.max_depth or len(np.unique(y)) == 1:\n",
                "            return {'class': np.bincount(y).argmax()}\n",
                "        \n",
                "        # Find best split\n",
                "        feature_idx, threshold = self.find_best_split(X, y)\n",
                "        \n",
                "        if feature_idx is None:\n",
                "            return {'class': np.bincount(y).argmax()}\n",
                "        \n",
                "        # Split data\n",
                "        X_left, X_right, y_left, y_right = self.split_data(X, y, feature_idx, threshold)\n",
                "        \n",
                "        # Build subtrees\n",
                "        return {\n",
                "            'feature': feature_idx,\n",
                "            'threshold': threshold,\n",
                "            'left': self.build_tree(X_left, y_left, depth + 1),\n",
                "            'right': self.build_tree(X_right, y_right, depth + 1)\n",
                "        }\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        \"\"\"Fit the decision tree.\"\"\"\n",
                "        self.tree = self.build_tree(X, y)\n",
                "        return self\n",
                "    \n",
                "    def predict_single(self, x, tree):\n",
                "        \"\"\"Predict a single sample.\"\"\"\n",
                "        if 'class' in tree:\n",
                "            return tree['class']\n",
                "        \n",
                "        if x[tree['feature']] < tree['threshold']:\n",
                "            return self.predict_single(x, tree['left'])\n",
                "        else:\n",
                "            return self.predict_single(x, tree['right'])\n",
                "    \n",
                "    def predict(self, X):\n",
                "        \"\"\"Predict multiple samples.\"\"\"\n",
                "        return np.array([self.predict_single(x, self.tree) for x in X])\n",
                "\n",
                "# Test custom implementation\n",
                "custom_dt = SimpleDecisionTree(max_depth=3)\n",
                "custom_dt.fit(X_train, y_train)\n",
                "y_pred_custom = custom_dt.predict(X_test)\n",
                "\n",
                "print(f\"Custom Decision Tree Accuracy: {accuracy_score(y_test, y_pred_custom):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### What You Learned:\n",
                "\n",
                "âœ… **Decision Tree Basics**\n",
                "- How decision trees split data\n",
                "- Gini impurity and entropy\n",
                "- Tree visualization and interpretation\n",
                "\n",
                "âœ… **Practical Skills**\n",
                "- Training decision tree classifiers and regressors\n",
                "- Feature importance analysis\n",
                "- Hyperparameter tuning\n",
                "- Preventing overfitting\n",
                "\n",
                "âœ… **Advanced Topics**\n",
                "- Cross-validation\n",
                "- Grid search for optimal parameters\n",
                "- Implementation from scratch\n",
                "\n",
                "### Key Takeaways:\n",
                "\n",
                "1. **Interpretability**: Decision trees are easy to understand and visualize\n",
                "2. **No Feature Scaling**: Don't need to normalize/standardize features\n",
                "3. **Overfitting Risk**: Prone to overfitting without proper constraints\n",
                "4. **Pruning**: Use max_depth, min_samples_split to control complexity\n",
                "5. **Versatile**: Works for both classification and regression\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. Try with different datasets (Titanic, Wine Quality, etc.)\n",
                "2. Experiment with different hyperparameters\n",
                "3. Compare with Random Forest (ensemble of trees)\n",
                "4. Learn about pruning techniques\n",
                "5. Explore gradient boosting algorithms\n",
                "\n",
                "**Happy Learning! ðŸŒ³**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}